{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "elon_bot_gpt2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNi9KeNRUbtoju83mUIWEXZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DJGJL04SLK1"
      },
      "source": [
        "# Automatic Elon Musk: Generating Synthetic Tweets Using GPT2\n",
        "\n",
        "GPT2 Finetuning using PyTorch and HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJaxpiytxKB9",
        "outputId": "00329a3f-b115-4540-dbaf-62f396fbe17d"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YI_M1sOwW0g"
      },
      "source": [
        "import torch.nn as nn\n",
        "import transformers\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, RandomSampler, SequentialSampler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random \n",
        "from google.colab import drive\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import transformers\n",
        "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import Trainer, TrainingArguments"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dUR5g9jRrPn"
      },
      "source": [
        "# empty cache to clear space for training\n",
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UMZ_JPpx7o7",
        "outputId": "93eeab3b-3607-482c-d8e8-a9c5db862a10"
      },
      "source": [
        "# mount drive\n",
        "drive.mount('/gdrive')\n",
        "drive_root = '/gdrive/My Drive/'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vhhh-liWx82r",
        "outputId": "2f56a266-6f70-4ea6-e23b-d6a1f5e61595"
      },
      "source": [
        "%cd ..\n",
        "%cd gdrive/MyDrive/elon_bot"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n",
            "/gdrive/MyDrive/elon_bot\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlEdePH5yDfM"
      },
      "source": [
        "# refer to elon_bot_lstm.ipynb \n",
        "init_tweets = pd.read_csv('tweets.csv')\n",
        "init_tweets = init_tweets['0']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpVQrLz9A41p"
      },
      "source": [
        "# ampersand bug\n",
        "tweets = []\n",
        "for tweet in init_tweets:\n",
        "  tweets.append(tweet.replace('&amp', '&'))\n",
        "\n",
        "tweets = pd.Series(tweets)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Xj80b4UyD4G",
        "outputId": "0c7067ee-dc83-4f95-ede0-a1e026d0ff89"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.3.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DBu9WF1Llh-",
        "outputId": "9b2fb0cd-0710-4b51-a25e-7bf428385746"
      },
      "source": [
        "# define batch size and load in pretrained tokenizer\n",
        "# adding bos, eos and pad token\n",
        "BATCH_SIZE=4\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZIM2FK1q-fW"
      },
      "source": [
        "# define Dataset class\n",
        "class TorchDataset(Dataset):\n",
        "\n",
        "  def __init__(self, tweets, tokenizer, max_length):\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "    self.input_ids = []\n",
        "    self.attn_mask = []\n",
        "\n",
        "    for tweet in tweets:\n",
        "      # tokenizing on a word level\n",
        "      # encodings is dictionary with two keys:\n",
        "      # one are the tokenized inputs, and other are attention_masks (what to pay attention to)\n",
        "      # all tweets are padded to length of 'max_length' w/ the pad token\n",
        "      # padded tokens are defaulted w/ attention 0\n",
        "      encodings = tokenizer('<|startoftext|>'+ tweet + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "      self.input_ids.append(torch.tensor(encodings['input_ids']))\n",
        "      self.attn_mask.append(torch.tensor(encodings['attention_mask']))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return {'input_ids': self.input_ids[idx], 'attn_mask': self.attn_mask[idx]}"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNOQykMuG6MP"
      },
      "source": [
        "# train test split\n",
        "dataset = TorchDataset(tweets, tokenizer, max_length=300)\n",
        "\n",
        "TRAIN_SIZE = int(0.85 * len(dataset))\n",
        "VAL_SIZE = len(dataset) - TRAIN_SIZE\n",
        "\n",
        "train_ds, val_ds = random_split(dataset, [TRAIN_SIZE, VAL_SIZE])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "fFdjhKWnH8Fm",
        "outputId": "ad1e88de-cf59-4fb6-dcd2-ed66403f04f5"
      },
      "source": [
        "# take look at one output\n",
        "tokenizer.decode((list(train_ds))[0]['input_ids'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<|startoftext|> I never used this guy. He gave a talk at SpaceX once.<|endoftext|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p41xP-RYS-v-"
      },
      "source": [
        "# DataLoader similar to tf dataset\n",
        "# train_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE) \n",
        "# for train, shuffle and batch randomly\n",
        "train_dataloader = DataLoader(train_ds, \n",
        "                              sampler = RandomSampler(train_ds), \n",
        "                              batch_size = BATCH_SIZE)\n",
        "\n",
        "# for validation can just batch sequentially.\n",
        "val_dataloader = DataLoader(val_ds,\n",
        "            sampler = SequentialSampler(val_ds),\n",
        "            batch_size = BATCH_SIZE)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxNSjqvWzUM-"
      },
      "source": [
        "from transformers import GPT2Tokenizer, TFGPT2Model\n",
        "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
        "\n",
        "# load pretrained model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
        "\n",
        "# added bos_token and eos_token to embeddings\n",
        "# need to resize otherwise tokenizer and model tensors won't match up\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model.cuda()\n",
        "\n",
        "# reproducability\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiJiFvBcz8_b"
      },
      "source": [
        "warmup_steps = 100\n",
        "epochs = 2\n",
        "# optmize model paramters with AdamW\n",
        "optimizer = AdamW(model.parameters(), lr = 0.001)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU9i9utMSI4u"
      },
      "source": [
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# change learning rate as throughout training loop\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = warmup_steps, \n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DY4Az0aVV6z"
      },
      "source": [
        "def train_epoch(model, dataloader, optimizer):\n",
        "  sample_every = 200\n",
        "  total_train_loss = 0\n",
        "  model.train()\n",
        "  for step, batch in enumerate(dataloader):\n",
        "\n",
        "    b_input_ids = batch['input_ids'].to(device)\n",
        "    b_labels = batch['input_ids'].to(device)\n",
        "    b_masks = batch['attn_mask'].to(device)\n",
        "\n",
        "    # zero gradients after each batch \n",
        "    # https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch \n",
        "    model.zero_grad()        \n",
        "\n",
        "    outputs = model(  b_input_ids,\n",
        "                      labels=b_labels, \n",
        "                      attention_mask = b_masks,\n",
        "                      token_type_ids=None\n",
        "                    )\n",
        "\n",
        "    loss = outputs[0]  \n",
        "    batch_loss = loss.item()\n",
        "    total_train_loss += batch_loss\n",
        "\n",
        "    # generate sentence evey n sample steps\n",
        "    if step % sample_every == 0 and not step == 0:\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        # https://huggingface.co/blog/how-to-generate\n",
        "        # top k: keep only top k words in distribution, and redistribute distribution to those words\n",
        "        # top p: filter words that exceed a probability p, then redistribute (dynamic!)\n",
        "        sample_outputs = model.generate(\n",
        "                                bos_token_id=random.randint(1,30000),\n",
        "                                do_sample=True,   \n",
        "                                top_k=50, \n",
        "                                max_length = 200,\n",
        "                                top_p=0.95, \n",
        "                                num_return_sequences=1\n",
        "                            )\n",
        "        for i, sample_output in enumerate(sample_outputs):\n",
        "              print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "        \n",
        "        model.train()\n",
        "\n",
        "    # calc loss gradients\n",
        "    loss.backward()\n",
        "    # update\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "  # average loss\n",
        "  return total_train_loss / len(dataloader)       "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlTRa_A4YgKj"
      },
      "source": [
        "def eval_epoch(model, dataloader):\n",
        "  model.eval()\n",
        "  val_loss = 0\n",
        "\n",
        "  # evaluate data for one epoch\n",
        "  for batch in dataloader:\n",
        "      \n",
        "      b_input_ids = batch['input_ids'].to(device)\n",
        "      b_labels = batch['input_ids'].to(device)\n",
        "      b_masks = batch['attn_mask'].to(device)\n",
        "      \n",
        "      # disable gradient calculation for evaluation\n",
        "      with torch.no_grad():        \n",
        "\n",
        "          outputs  = model(b_input_ids, attention_mask = b_masks, labels=b_labels)\n",
        "          loss = outputs[0]  \n",
        "          \n",
        "      batch_loss = loss.item()\n",
        "      val_loss += batch_loss        \n",
        "\n",
        "  return val_loss / len(dataloader)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PjMNzHVjJZ4",
        "outputId": "bac0ba71-22e6-4ac9-c4d3-2420593a76b6"
      },
      "source": [
        "# train and generate every 200 steps\n",
        "model = model.to(device)\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    train_loss = train_epoch(model, train_dataloader, optimizer)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(train_loss))\n",
        "\n",
        "    val_loss = eval_epoch(model, val_dataloader)\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(val_loss))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 2 ========\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0:  invitesThere is a long lasting lesson in a few weeks\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0:  CCTesla Model X, in Japan.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0:  barbarAnd, as soon as we get the upper stage (or the main engine) at night we will be at the top.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0: ramerThat's why. It’s like a Tesla. Aiming at real, product vs market, but just an increase in production.\n",
            "\n",
            "  Average training loss: 0.45\n",
            "  Validation Loss: 0.24\n",
            "\n",
            "======== Epoch 2 / 2 ========\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0:  NeuroIt is better to be good as a product than as a service.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0:  hardSpaceX is trying to make Falcon 9 reusable by end of the year, but we still need 3. Starship has a rocket?\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0:  enthusI will be reviewing the latest Tesla Model S/X/X SUV unveil on Tesla website every Monday at 3pm California time. Also a few things to clarify to all that's coming.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0:  BryantTbh like that\n",
            "\n",
            "  Average training loss: 0.18\n",
            "  Validation Loss: 0.24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KaBp9LCDTjQ",
        "outputId": "611e13af-b432-470d-9424-3b3e3344f797"
      },
      "source": [
        "# print out text\n",
        "model.eval()\n",
        "# start string of start token\n",
        "prompt = \"<|startoftext|>\"\n",
        "\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "generated = generated.to(device)\n",
        "\n",
        "print(generated)\n",
        "\n",
        "sample_outputs = model.generate(\n",
        "                                generated, \n",
        "                                do_sample=True,   \n",
        "                                top_k=50, \n",
        "                                max_length = 300,\n",
        "                                num_return_sequences=3\n",
        "                                )\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([[50257]], device='cuda:0')\n",
            "0: Thanks, we can't wait to see thousands of Model 3 cars rolling out at launch. Launch dates would be April 1st and April 31st.\n",
            "\n",
            "\n",
            "1: Dogecoin is coming all over again. Selling at this rate, so much more incentive flow to incent purchase.\n",
            "\n",
            "\n",
            "2: yes\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVaEmzoPIrHa"
      },
      "source": [
        "## References:\n",
        "\n",
        "\n",
        "*   HuggingFace https://huggingface.co/transformers/model_doc/gpt2.html\n",
        "*   Rey Farhan http://reyfarhan.com/posts/easy-gpt2-finetuning-huggingface/ (heavily inspired and sourced from his tutorial)\n",
        "*   PyTorch Datasets https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "\n"
      ]
    }
  ]
}